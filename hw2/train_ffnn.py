# -*- coding: utf-8 -*-
"""train_ffnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hjhNgqjiLxp5XyWk6-WwQUMhJpQSfg1v
"""

import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
import multiprocessing as mp
from datetime import datetime
import matplotlib.pyplot as plt
import time
from pdb import set_trace as bp
torch.manual_seed(100)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

def LoadTrainData(filename):
    file = open(filename, 'r', encoding='utf8')
    content_list = file.read().split(" ")
    word2int = {}
    ix = 0
    for word in content_list:
        if word not in word2int:
            word2int[word]=ix
            ix+=1
            
    encoded_list = []
    for word in content_list:
        encoded_list.append(word2int[word])
    
    return(encoded_list, word2int)

def LoadTestData(filename, word2int):
    file = open(filename, 'r', encoding='utf8')
    content_list = file.read().split(" ")
    encoded_list = []
    for word in content_list:
        if word in word2int:
            encoded_list.append(word2int[word])
        else:
            encoded_list.append(word2int['<unk>'])
    
    return encoded_list


class WikiDataset(torch.utils.data.Dataset):
    def __init__(self, data, window):
        self.data = data
        self.window = window
        
    def __getitem__(self, index):
        x = torch.tensor(self.data[index:index+self.window]) #x = the five context words
        y = torch.tensor(self.data[index+self.window+1]) #y = the target word to be predicted
        return x,y
    
    def __len__(self):
        return len(self.data)-self.window-1


class FFNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim, device = device):
        super(FFNN, self).__init__()
        self.context_size = context_size
        self.embedding_dim = embedding_dim 
        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size 

        # embedding (input tied with output)
        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)
        self.linear1 = nn.Linear(self.embedding_dim * self.context_size, self.hidden_dim, bias = True)
        self.linear2 = nn.Linear(self.hidden_dim, self.vocab_size, bias = True)
        init_weights(self)
        self.linear2.weight = self.embeddings.weight
    
        
        
    def forward(self, inputs):
        # originally (words_input, embedding_dimension), but reshape to (1 x (words_input * embedding_dimension))
        embedding = self.embeddings(inputs).view((-1, self.embedding_dim * self.context_size))
        output_linear1 = torch.tanh(self.linear1(embedding))
        output_linear2 = self.linear2(output_linear1)

        #return F.softmax(output_linear2, dim = 1)
        return output_linear2

def init_weights(m):
	for p in m.parameters():
		p = nn.init.uniform_(p, a=-0.1, b=0.1)
  
def prepare_loader(dataset, batch_size, num_workers):
    return torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers, drop_last=True)


def get_accuracy_from_log_probs(log_probs, labels):
    probs = torch.exp(log_probs)
    predicted_label = torch.argmax(probs, dim=1)
    acc = (predicted_label == labels).float().mean()
    return acc

def evaluate(model, loss_func, dataloader):
    model.eval()

    mean_acc, mean_loss = 0, 0
    count = 0

    with torch.no_grad():
        dev_st = time.time()
        for i, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)
            log_probs = model(X)
            mean_loss += loss_func(log_probs, y).item()
            mean_acc += get_accuracy_from_log_probs(log_probs, y)
            count += 1
    return mean_acc / count, mean_loss / count

"""# New Section"""

loadtrain, word_to_int_dict = LoadTrainData('wiki.train.txt')
loadvalid = LoadTestData('wiki.valid.txt', word_to_int_dict)
loadtest = LoadTestData('wiki.test.txt', word_to_int_dict)

#specify some parameters
window_size=5
vocab_size = len(word_to_int_dict)
embedding_dim_size = 100
hidden_dim_size = 100

#load data
train_data = WikiDataset(loadtrain, window_size)
valid_data = WikiDataset(loadvalid, window_size)
test_data = WikiDataset(loadtest, window_size)

train_loader = prepare_loader(train_data, batch_size=20, num_workers=mp.cpu_count())
test_loader = prepare_loader(test_data, batch_size=20, num_workers=mp.cpu_count())
valid_loader = prepare_loader(valid_data, batch_size = 20, num_workers = mp.cpu_count())
#define model
model = FFNN(vocab_size, embedding_dim_size, window_size, hidden_dim_size)


model.to(device)

# model.apply(init_weights)
# for p in model.parameters():
#   p.requires_grad = True
#specify some more parameters
loss_func = nn.CrossEntropyLoss()
learning_rate = .0001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
epochs = 20

#run training and store loss values
train_loss = []
test_loss = []
val_loss = []

best_model_path = ''
last_embedding_weight = model.embeddings.state_dict()["weight"]
for epoch in range(1, 21):
    st = time.time()
    print("\n--- Training model Epoch: {} ---".format(epoch))
    tra_loss = 0
    for i, (X, y) in enumerate(train_loader): 
        weights = []
        for param in model.parameters():
            weights.append(param.clone())
        model.train()
        X, y = X.to(device), y.to(device)

        # zero out the gradients from the old instance
        optimizer.zero_grad()

        # get log probabilities over next words
        pred = model(X)

        # calculate current accuracy
        
        acc = get_accuracy_from_log_probs(pred, y)
        # compute loss function
        loss = loss_func(pred, y)

        # backward pass and update gradient
        loss.backward()

        optimizer.step()

        tra_loss = loss
        weights_after_backprop = [] # weights after backprop
        for param in model.parameters():
            weights_after_backprop.append(param.clone()) # only layer1's weight should update, layer2 is not used

        if i % 5000 == 0: 
            print("Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}".format(i, epoch, loss.item(), acc, (time.time()-st)))
            st = time.time()

            weights_equivalence = [torch.equal(z[0], z[1]) for z in zip(weights, weights_after_backprop)]
            
            if all(weights_equivalence):
              print("weight hasn't changed")
            else:
              print("weight has changed")
            last_embedding_weight = model.embeddings.state_dict()["weight"]
    print("\n--- Evaluating model on dev data ---")
    dev_acc, dev_loss = evaluate(model, loss_func, valid_loader)
    tes_acc, tes_loss = evaluate(model, loss_func, test_loader)
    val_loss.append(dev_loss)
    test_loss.append(tes_loss)
    train_loss.append(tra_loss)
    print("Epoch {} complete! Development Accuracy: {}; Development Loss: {}".format(epoch, dev_acc, dev_loss))
    # set best model path
    best_model_path = 'best_model_{}.dat'.format(epoch)
    # saving best model
    torch.save(model.state_dict(), best_model_path)

#chris debug
train_loss = []
test_loss = []
val_loss = []

best_model_path = ''
last_embedding_weight = model.embeddings.state_dict()["weight"]
for epoch in range(1, 21):
    st = time.time()
    print("\n--- Training model Epoch: {} ---".format(epoch))
    tra_loss = 0
    for i, (X, y) in enumerate(train_loader): 
        weights = []
        for param in model.parameters():
            weights.append(param.data.detach().clone())
        model.train()
        X, y = X.to(device), y.to(device)

        # get log probabilities over next words
        pred = model(X)

        # calculate current accuracy
        
        acc = get_accuracy_from_log_probs(pred, y)
        # compute loss function
        loss = loss_func(pred, y)

        # backward pass and update gradient
        loss.backward()
        for n, p in model.named_parameters():
          print('param for ' + n, p.data)
          input("hi")
        optimizer.step()

        # zero out the gradients from the old instance
        optimizer.zero_grad()

        tra_loss = loss
        weights_after_backprop = [] # weights after backprop
        for param in model.parameters():
            weights_after_backprop.append(param.data.detach().clone()) # only layer1's weight should update, layer2 is not used

        if i % 10 == 0 and i != 0: 
            print("Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}".format(i, epoch, loss.item(), acc, (time.time()-st)))
            st = time.time()

            weights_equivalence = [torch.equal(z[0], z[1]) for z in zip(weights, weights_after_backprop)]
            
            if all(weights_equivalence):
              print("weight hasn't changed")
            else:
              print("weight has changed")
            last_embedding_weight = model.embeddings.state_dict()["weight"]
    print("\n--- Evaluating model on dev data ---")
    dev_acc, dev_loss = evaluate(model, loss_func, valid_loader)
    tes_acc, tes_loss = evaluate(model, loss_func, test_loader)
    val_loss.append(dev_loss)
    test_loss.append(tes_loss)
    train_loss.append(tra_loss)
    print("Epoch {} complete! Development Accuracy: {}; Development Loss: {}".format(epoch, dev_acc, dev_loss))
    # set best model path
    best_model_path = 'best_model_{}.dat'.format(epoch)
    # saving best model
    torch.save(model.state_dict(), best_model_path)

